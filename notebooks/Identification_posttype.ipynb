{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DiDw3_Evrs69"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the data into a pandas DataFrame\n",
        "df = pd.read_csv('consolidatedmh.csv')\n",
        "\n",
        "df.fillna(\"\", inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mapping = {\n",
        "    \"General Discussion / Questions / Support\": [\n",
        "        \"Opinion / Thought / Idea / Discussion\",\n",
        "        \"Seeking Support\",\n",
        "        \"Undiagnosed Questions\",\n",
        "        \"Questions/Advice/Support\",\n",
        "        \"Seeking Empathy / Support\",\n",
        "        \"General Question About BP\",\n",
        "        \"General Discussion\",\n",
        "        \"Community Discussion\",\n",
        "        \"Question\",\n",
        "        \"Discussion\",\n",
        "        \"User in need of help that I have no relation to\",\n",
        "        \"is this actually a problem and if so where do i go to fix it?\",\n",
        "        \"is it okay to do this?\",\n",
        "        \"is there something seriously wrong with me\",\n",
        "        \"what does it mean girl is in love with psychopath/sociopath?\",\n",
        "        \"why am i like this?\",\n",
        "        \"wtf do i do? \",\n",
        "        \"shame and guilt\",\n",
        "        \"should i be worried about my mom?\",\n",
        "        \"what is this called??\",\n",
        "        \"wtf do i do next???\",\n",
        "        \"what does this mean?\",\n",
        "        \"how to gett rid of obsession?\",\n",
        "        \"dealing with pro-russian voice in my head\",\n",
        "        \"am i just messed up or is there hope\",\n",
        "        \"anyone relate ?\",\n",
        "        \"distorted fantasy\",\n",
        "        \"dae\",\n",
        "        \"positives\",\n",
        "        \"harm reduction\",\n",
        "        \"lgbtq+\",\n",
        "    ],\n",
        "    \"Rant / Vent\": [\n",
        "        \"Rant / Vent\",\n",
        "        \"Vent\",\n",
        "        \"VENT / RANT\",\n",
        "        \"Vent (comments welcome)\",\n",
        "        \"Venting\",\n",
        "        \"Rant/Story\",\n",
        "        \"Manic Rant\",\n",
        "        \"Rant/Discussion\",\n",
        "        \"Question/Venting\",\n",
        "        \"Question/Need Support\",\n",
        "        \"Question/Advice\",\n",
        "        \"Venting/needing some help\",\n",
        "        \"Venting Post\",\n",
        "        \"Therapy Rant\",\n",
        "        \"rant\",\n",
        "        \"story\",\n",
        "    ],\n",
        "    \"Advice / Encouragement\": [\n",
        "        \"Advice / Encouragement\",\n",
        "        \"Advice Needed\",\n",
        "        \"Advice to Give\",\n",
        "        \"Tips/Suggestions\",\n",
        "        \"Advice/Support\",\n",
        "        \"Happiness &amp; Positivity\",\n",
        "        \"Encouragement\",\n",
        "        \"Encouragement - Story Time\",\n",
        "        \"Input\",\n",
        "        \"Perspective Needed\",\n",
        "        \"Helpful Tips!\",\n",
        "        \"Help\",\n",
        "        \"Needs A Hug/Support\",\n",
        "        \"Support\",\n",
        "        \"Seeking Support &amp; Advice\",\n",
        "        \"üí≠Seeking Support &amp; Advice\",\n",
        "        \"obsessions about a person\",\n",
        "    ],\n",
        "    \"Medication / Medical / Treatment\": [\n",
        "        \"Medication\",\n",
        "        \"Medications\",\n",
        "        \"Therapist / Doctors\",\n",
        "        \"Vitamin/Supplement\",\n",
        "        \"Medical Study\",\n",
        "        \"Med Question\",\n",
        "        \"Medication Discussion\",\n",
        "        \"Question About Meds\",\n",
        "        \"Question about Meds\",\n",
        "        \"Medical Advice\",\n",
        "        \"Medicine\",\n",
        "        \"therapy\",\n",
        "    ],\n",
        "    \"Work / School / Relationship / Lifestyle\": [\n",
        "        \"Work / School\",\n",
        "        \"Relationships\",\n",
        "        \"Pro Tip\",\n",
        "        \"Food and recipes\",\n",
        "        \"Driving\",\n",
        "        \"Lifestyle\",\n",
        "        \"Travel\",\n",
        "        \"Relationship/Love Issues\",\n",
        "        \"Family/Friendship Issues\",\n",
        "        \"School/Workplace Issues\",\n",
        "        \"Health\",\n",
        "        \"Work/School\",\n",
        "        \"Family/Relationship\",\n",
        "        \"Friend/Family\",\n",
        "        \"SOS!\",\n",
        "        \"Relationships \",\n",
        "        \"Selfie \",\n",
        "        \"Reminder\",\n",
        "        \"divorce\",\n",
        "        \"loneliness\",\n",
        "        \"loss of a loved one\",\n",
        "        \"financial issues\",\n",
        "        \" self esteem issues\",\n",
        "        \"flair\",\n",
        "    ],\n",
        "\"Progress / Positivity / Success\": [\n",
        "    \"Progress / Good News ‚òÄÔ∏è\",\n",
        "    \"Success/Celebration\",\n",
        "    \"Accountability\",\n",
        "    \"Success Story\",\n",
        "    \"Small Triumph\",\n",
        "    \"Recovery Story\",\n",
        "    \"Progress!\",\n",
        "    \"Lesson Learned\",\n",
        "    \"Positivity\",\n",
        "    \"Happy!\",\n",
        "    \"Success!\",\n",
        "    \"Share Your Victories\",\n",
        "    \"üíñPositivity &amp; Affirmation Post\",\n",
        "    \"üåüSuccess Story/Small Triumph\",\n",
        "    \"Over 30\",\n",
        "    \"Uplifting\",\n",
        "    \"Anxiety Resource\",\n",
        "    \"Sleep\",\n",
        "    \"DAE Questions\",\n",
        "    \"Progress Post\",\n",
        "    \"Good News / Happy\",\n",
        "],\n",
        "\"Art / Media / Entertainment\": [\n",
        "    \"Art\",\n",
        "    \"Video\",\n",
        "    \"Meme\",\n",
        "    \"Music\",\n",
        "    \"Original Art\",\n",
        "    \"Non-Original Art\",\n",
        "    \"I Made This!\",\n",
        "    \"Humor\",\n",
        "    \"Humour\",\n",
        "    \"Art/Meme\",\n",
        "    \"Infographic\",\n",
        "    \"Mortal Danger\",\n",
        "    \"Guess the Meds\",\n",
        "],\n",
        "\"Resources / Literature / Research\": [\n",
        "    \"Resources\",\n",
        "    \"Literature\",\n",
        "    \"Research\",\n",
        "    \"Scientific Article\",\n",
        "    \"Educational Resource\",\n",
        "    \"Reference Material\",\n",
        "    \"Scientific Research\",\n",
        "    \"Peer-Reviewed Study\",\n",
        "    \"resource\",\n",
        "    \"clinical psychology study\",\n",
        "],\n",
        "\"Mental Health Issues / Symptoms\": [\n",
        "    \"Mental Health\",\n",
        "    \"Symptoms\",\n",
        "    \"Depression\",\n",
        "    \"delusions\",\n",
        "    \"Anxiety\",\n",
        "    \"Mental Health Issue\",\n",
        "    \"Mood Disorder\",\n",
        "    \"PTSD\",\n",
        "    \"OCD\",\n",
        "    \"Eating Disorder\",\n",
        "    \"ADHD\",\n",
        "    \"Borderline Personality Disorder\",\n",
        "    \"BPD\",\n",
        "    \"hallucinations / delusions\",\n",
        "    \"hallucinations\",\n",
        "    \"disorganized thoughts\",\n",
        "    \"tobacco / alcohol / drugs\",\n",
        "    \"obsession sharing!\",\n",
        "    \"suicidal thoughts\",\n",
        "    \"tactile/auditory hallucinations\",\n",
        "],\n",
        "\"Trigger Warning / Content Warning\": [\n",
        "    \"Trigger Warning\",\n",
        "    \"Content Warning\",\n",
        "    \"TW\",\n",
        "    \"CW\",\n",
        "    \"mention of suicide\",\n",
        "    \"suicide\",\n",
        "    \"suicidal\",\n",
        "    \"undiagnosed\",\n",
        "    \"self harm\",\n",
        "    \"undiagnosed, suicide, self-harm and loss of a loved one\",\n",
        "    \"dangerous behavior warning\",\n",
        "    \"dangerous behavior\",\n",
        "    \"nsfw\",\n",
        "],\n",
        "\"Introduction / New Member / Community\": [\n",
        "    \"Introduction\",\n",
        "    \"New Member\",\n",
        "    \"Community\",\n",
        "    \"New User\",\n",
        "    \"Hello\",\n",
        "    \"New Here\",\n",
        "    \"New to the Sub\",\n",
        "    \"Newbie\",\n",
        "    \"First Post\",\n",
        "    \"oops, i did it... again\",\n",
        "    \"urgent: coping skills needed\",\n",
        "    \"baby borderline\",\n",
        "    \"new coping skill achievement unlocked!\",\n",
        "    \"radical acceptance\",\n",
        "    \"quiet borderline\",\n",
        "    \"acted opposite to emotion\",\n",
        "    \"it's not the end of the world\",\n",
        "    \"veteran borderline\",\n",
        "    \"üí¨general post\",\n",
        "    \"üí¨general dbt post\",\n",
        "    \"dae?\",\n",
        "],\n",
        "\"Mod Announcement / Official\": [\n",
        "    \"Mod Announcement\",\n",
        "    \"Official\",\n",
        "    \"Admin\",\n",
        "    \"Rules\",\n",
        "    \"Moderator\",\n",
        "    \"mod post\",\n",
        "    \"üåømj üåø\",\n",
        "    \"announcement\",\n",
        "    \"guest ama\",\n",
        "    \"monthly town hall\",\n",
        "    \"meta\",\n",
        "    \"drug use\",\n",
        "    \"war veterans \",\n",
        "    \"sadness / grief\",\n",
        "    \"opinion / thoughts\",\n",
        "    \"feelings friday\",\n",
        "    \"member poll \",\n",
        "    \"anger issues\",\n",
        "    \"test\",\n",
        "    \"hospitalization\",\n",
        "    \"just sharing\",\n",
        "    \"megathread: weekly wins\",\n",
        "    \"megathread: short posts\",\n",
        "    \"megathread: newly diagnosed\",\n",
        "]\n",
        "}"
      ],
      "metadata": {
        "id": "2Waas7acP5OJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_flair_to_key_text(flair):\n",
        "    if pd.isna(flair) or flair == '':\n",
        "        return \"Other\"\n",
        "    flair = flair.lower()\n",
        "    for key, flair_list in mapping.items():\n",
        "        for flair_item in flair_list:\n",
        "            if flair_item.lower() in flair:\n",
        "                return key\n",
        "    if \"tw\" in flair or \"cw\" in flair:\n",
        "        return \"Trigger Warning / Content Warning\"\n",
        "    if \"question\" in flair or \"advice\" in flair:\n",
        "        return \"General Discussion / Questions / Support\"\n",
        "    if \"üö∂üíÄ\" in flair:\n",
        "        return \"General Discussion / Questions / Support\"\n",
        "    if \"snoo\" in flair:\n",
        "        return \"General Discussion / Questions / Support\"\n",
        "    if \"need support\" in flair:\n",
        "        return \"General Discussion / Questions / Support\"\n",
        "    return \"Other\""
      ],
      "metadata": {
        "id": "0z9TV_BLQG5q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, data_point in df.iterrows():\n",
        "    df.at[index, \"map_flair\"] = map_flair_to_key_text(data_point[\"Flair\"])"
      ],
      "metadata": {
        "id": "rhVXoiyfQIxE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all unique values of the 'flair' column\n",
        "unique_flairs = df['map_flair'].unique().tolist()\n",
        "\n",
        "# Print the results\n",
        "print(unique_flairs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSqW06bXQnVH",
        "outputId": "bd30147f-adc1-4340-e85d-f6c9e8fd45dd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['General Discussion / Questions / Support', 'Rant / Vent', 'Medication / Medical / Treatment', 'Work / School / Relationship / Lifestyle', 'Advice / Encouragement', 'Trigger Warning / Content Warning', 'Mental Health Issues / Symptoms', 'Introduction / New Member / Community', 'Progress / Positivity / Success', 'Art / Media / Entertainment', 'Resources / Literature / Research', 'Mod Announcement / Official', 'Other']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(df[df['map_flair'] == 'specific'].index)"
      ],
      "metadata": {
        "id": "V10FwIZqIiLR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install embedders transformers"
      ],
      "metadata": {
        "id": "KMunhMNbqNeM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b6b84d9-e95a-4ec7-9c80-f34838fcfe17"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: embedders in /usr/local/lib/python3.9/dist-packages (0.0.19)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.4)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from embedders) (2.0.0+cu118)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from embedders) (1.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from embedders) (1.22.4)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.9/dist-packages (from embedders) (2.2.2)\n",
            "Requirement already satisfied: spacy>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from embedders) (3.5.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from embedders) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->embedders) (0.7.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->embedders) (2.4.6)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->embedders) (1.10.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->embedders) (2.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->embedders) (67.6.1)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->embedders) (1.1.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->embedders) (3.0.8)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->embedders) (0.10.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->embedders) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->embedders) (1.0.4)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->embedders) (8.1.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->embedders) (2.0.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->embedders) (1.0.9)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->embedders) (3.3.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->embedders) (6.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->embedders) (3.1.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->embedders) (3.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->embedders) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->embedders) (2.0.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->embedders) (16.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->embedders) (3.25.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->embedders) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->embedders) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->embedders) (1.1.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from sentence-transformers->embedders) (0.15.1+cu118)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from sentence-transformers->embedders) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (from sentence-transformers->embedders) (0.1.97)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.0.0->embedders) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.0.0->embedders) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy>=3.0.0->embedders) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy>=3.0.0->embedders) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.6.0->embedders) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->sentence-transformers->embedders) (8.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression spits out warnings on large datasets. For now, there warning will be surpressed.\n",
        "# This warning will get taken care of in a later version\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Basis libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import sys\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from configparser import ConfigParser\n",
        "from pathlib import Path\n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "\n",
        "# Sklearn libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    confusion_matrix,\n",
        "    mean_squared_error,\n",
        ")\n",
        "\n",
        "# Embedders and Transformers\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from embedders.classification.contextual import TransformerSentenceEmbedder"
      ],
      "metadata": {
        "id": "bW51cHv5WIqS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_getter(df):\n",
        "    \"\"\"\n",
        "    Function to grab and validate multiple column names from user.\n",
        "    Input should be a pandas DataFrame.\n",
        "\n",
        "    Args:\n",
        "    df -> An already loaded pandas DataFrame.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        try:\n",
        "            features_input = \"Title, Text\"\n",
        "            features_cleaned = [i.strip() for i in features_input.split(sep=\",\")]\n",
        "            if pd.Series(features_cleaned).isin(df.columns).all():\n",
        "                break\n",
        "            else:\n",
        "                pass\n",
        "        except:\n",
        "            pass\n",
        "    return features_cleaned\n",
        "\n",
        "def target_getter(df):\n",
        "    \"\"\"\n",
        "    Function to grab and validate a single column name from user.\n",
        "    Input should be a pandas DataFrame.\n",
        "\n",
        "    Args:\n",
        "    df -> An already loaded pandas DataFrame.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        try:\n",
        "            target_input = \"map_flair\"\n",
        "            if target_input in df:\n",
        "                break\n",
        "            else:\n",
        "                pass  \n",
        "        except:\n",
        "            pass\n",
        "    return target_input"
      ],
      "metadata": {
        "id": "jBTxi6XlotmR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use cuda if available \n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# get datetime dd/mm/YY H:M\n",
        "now = datetime.now()\n",
        "dt_string = now.strftime(\"%d-%m-%Y %H-%M\")"
      ],
      "metadata": {
        "id": "H6fG1LNLpKTW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_columns = feature_getter(df)\n",
        "\n",
        "corpus = df[feature_columns]\n",
        "if len(corpus.columns) > 1:\n",
        "    corpus = corpus[corpus.columns].apply(\n",
        "        lambda x: \",\".join(x.dropna().astype(str)), axis=1\n",
        "    )\n",
        "    corpus = corpus.tolist()\n",
        "else:\n",
        "    corpus = corpus.squeeze().tolist()"
      ],
      "metadata": {
        "id": "dgXpUnnBpnLe"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the names of the labels\n",
        "target_column = target_getter(df)\n",
        "target = df[target_column].values\n",
        "\n",
        "# Identify if the labels are strings and need encodig\n",
        "if target.dtype == 'O' or str:\n",
        "    encoder = LabelEncoder()\n",
        "    target = encoder.fit_transform(target)\n",
        "    encoder_usage = True\n",
        "else:\n",
        "    encoder_usage = False"
      ],
      "metadata": {
        "id": "dAps4VInpzcZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"distilbert-base-uncased\"\n",
        "sent_transformer = TransformerSentenceEmbedder(model_name)\n",
        "word_embeddings = sent_transformer.transform(corpus)\n",
        "embeddings = np.array(word_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGNNu0Iwp8ZQ",
        "outputId": "375ae5cc-a20b-4e2e-dfab-9985b22f8ce2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "Some weights of the model checkpoint at /root/.cache/torch/sentence_transformers/distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing model, might take some time...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoding batches ...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1821/1821 [30:06<00:00,  1.01it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up features and labels\n",
        "features =  torch.FloatTensor(embeddings).to(device)\n",
        "target = torch.LongTensor(target).to(device)\n",
        "\n",
        "# Splitting the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
        "\n",
        "try:\n",
        "    num_classes = len(encoder.classes_)\n",
        "except:\n",
        "    num_classes = len(df[target_column].value_counts())"
      ],
      "metadata": {
        "id": "ZS5ydRUwrMVd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Linear(X_train.shape[1], 512),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm1d(512),\n",
        "    nn.Dropout(p=0.3),\n",
        "\n",
        "    nn.Linear(512, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm1d(256),\n",
        "    nn.Dropout(p=0.3),\n",
        "\n",
        "    nn.Linear(256, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm1d(128),\n",
        "    nn.Dropout(p=0.3),\n",
        "\n",
        "    nn.Linear(128, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm1d(64),\n",
        "    nn.Dropout(p=0.3),\n",
        "\n",
        "    nn.Linear(64, num_classes)\n",
        ").to(device)\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.01)"
      ],
      "metadata": {
        "id": "ZUV0KSlIriuH"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "print(\" \")\n",
        "print(\"Training model...\")\n",
        "\n",
        "# Training loop of the model\n",
        "num_epochs = 5000 \n",
        "for epoch in range(num_epochs):\n",
        "    # Create prediction of the model\n",
        "    y_hat = model(X_train)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = loss_function(y_hat, y_train)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    # Backpropagate to update weights\n",
        "    model.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    #if epoch % 10 == 0:\n",
        "    sys.stdout.write(f'\\rLoss of {round(float(loss), 2)} at epoch {epoch+1} of {num_epochs}')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyPZfKIurm5H",
        "outputId": "6da71e9f-7725-4b2b-8717-a7a52678c6ca"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "Training model...\n",
            "Loss of 0.54 at epoch 5000 of 5000"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Save model\n",
        "    torch.save(model, \"ml/model.pt\")\n",
        "\n",
        "    # Save encoder\n",
        "    with open(\"ml/encoder.pkl\", \"wb\") as handle:\n",
        "        pickle.dump(encoder, handle)\n",
        "\n",
        "except:\n",
        "        # Save model\n",
        "    torch.save(model.state_dict(), \"model.pt\")\n",
        "\n",
        "    # Save encoder\n",
        "    with open(\"encoder.pkl\", \"wb\") as handle:\n",
        "        pickle.dump(encoder, handle)"
      ],
      "metadata": {
        "id": "CeDq--Zir7FM"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = []\n",
        "model.eval()\n",
        "for i in X_test:\n",
        "    pred = model(i.unsqueeze(0))\n",
        "    pred_numpy = pred.cpu().detach().numpy().astype(int)\n",
        "    y_pred.append(np.argmax(pred_numpy))\n",
        "\n",
        "# COnvert y_test to a numpy array\n",
        "y_test = y_test.cpu().numpy()"
      ],
      "metadata": {
        "id": "qz3i31lOGmVf"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate evaluation metrics\n",
        "print(\" \")\n",
        "print(\"Generating evaluation metrics on unseen testing data...\")\n",
        "print(\"- - - - - - - - - - - - - - - -\")\n",
        "print(f\"Model accuracy is: {round(accuracy_score(y_test, y_pred), 2) * 100} %\")\n",
        "print(\"- - - - - - - - - - - - - - - -\")\n",
        "print(f\"Mean squared error is: {round(np.sqrt(mean_squared_error(y_test, y_pred)), 2)}\")\n",
        "print(\"- - - - - - - - - - - - - - - -\")\n",
        "print(f\"The confusion matrix is:\")\n",
        "for row in confusion_matrix(y_test, y_pred):\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PXWVoXQr_Ak",
        "outputId": "6a57e60c-e992-453c-8afa-a398168574a5"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "Generating evaluation metrics on unseen testing data...\n",
            "- - - - - - - - - - - - - - - -\n",
            "Model accuracy is: 56.00000000000001 %\n",
            "- - - - - - - - - - - - - - - -\n",
            "Mean squared error is: 3.59\n",
            "- - - - - - - - - - - - - - - -\n",
            "The confusion matrix is:\n",
            "[2261   22 1466    5   80    9   17 1258   48  316    5   15   62]\n",
            "[ 42 155  70   4   2   1  14  91  11  14   3   1  11]\n",
            "[2114   24 7347   18  159   13   52 1656   86  440    4   15   53]\n",
            "[ 73   2 150  12   5   1   3  85   4  48   2   1   2]\n",
            "[254   4 274   0 326   2   9 155   8  16   2   1   6]\n",
            "[ 70   5 164   4   2  21   3 114   6  29   0   2   6]\n",
            "[182  22 138   0   8   3  44 312  12  56   2   1   4]\n",
            "[ 1779    66  1542    11   125    11    72 14830    67   888    17    33\n",
            "    47]\n",
            "[163  13 247   5  11   1  12 166 169  55   0   1   9]\n",
            "[ 691    7  500    7    9    2   17 1400   27  924    1   14   25]\n",
            "[18  6 25  1  1  0  2 46  5  3 15  1  2]\n",
            "[132   7 141   2   6   0   6 282   7  83   0  14   3]\n",
            "[455   7 185   3  15   7   2 273  25  86   0   2 144]\n"
          ]
        }
      ]
    }
  ]
}