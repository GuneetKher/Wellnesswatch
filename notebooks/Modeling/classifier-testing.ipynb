{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae0e44ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at model were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at model and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at model were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at model and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, XLMRobertaForSequenceClassification\n",
    "import torch\n",
    "\n",
    "base_model1 = XLMRobertaForSequenceClassification.from_pretrained(\"model\",num_labels=2,from_tf=False,local_files_only=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"model\",local_files_only=True)\n",
    "base_model2 = XLMRobertaForSequenceClassification.from_pretrained(\"model\",num_labels=2,from_tf=False,local_files_only=True)\n",
    "\n",
    "\n",
    "class ClassPredictor:\n",
    "  def __init__(self,model,tokenizer,model_state):\n",
    "    self.model = model\n",
    "    self.tokenizer = tokenizer\n",
    "    # self.model.load_state_dict(torch.load('/content/drive/MyDrive/roberta_model.pt'))\n",
    "    self.model.load_state_dict(torch.load(model_state))\n",
    "    self.model.eval()\n",
    "\n",
    "  def __call__(self, text, prob=True):\n",
    "    if isinstance(text, str):\n",
    "      text = [text]\n",
    "      unpack = True\n",
    "    else:\n",
    "      unpack = False\n",
    "\n",
    "    tokenized_text = [self.tokenizer.encode(t, add_special_tokens=True) for t in text]\n",
    "    # Pad the tokenized input text to the same length\n",
    "    max_length = max(len(t) for t in tokenized_text)\n",
    "    padded_text = [t + [0] * (max_length - len(t)) for t in tokenized_text]\n",
    "    # Convert the padded input text to PyTorch tensor\n",
    "    input_ids = torch.tensor(padded_text)\n",
    "    # Make predictions with the model\n",
    "    with torch.no_grad():\n",
    "      logits = self.model(input_ids).logits\n",
    "      probs = torch.softmax(logits, dim=1)\n",
    "#       print(logits.argmax())\n",
    "        \n",
    "\n",
    "\n",
    "    if prob:\n",
    "      if unpack:\n",
    "        return (bool(logits.argmax()),probs[0,1].tolist())\n",
    "      else:\n",
    "        return (bool(logits.argmax()),probs[:,1].tolist())\n",
    "    else:\n",
    "      return bool(logits.argmax())\n",
    "\n",
    "cp = ClassPredictor(base_model1,tokenizer,'roberta_model.pt')\n",
    "sp = ClassPredictor(base_model2,tokenizer,'roberta_model_strict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68b98a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "I know I'm not like everyone else. I don't fit in, and I don't know how to act around people. Sometimes, it feels like I'm an outsider, looking in. But it's not like I want to be this way. I wish I could be normal, and have friends, and go to parties, and not feel so alone all the time.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80612052",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(True, 0.9996656179428101) (True, 0.9987581968307495)\n"
     ]
    }
   ],
   "source": [
    "print(cp(text),sp(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4dfb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp(\"Did anyone catch the cannucks' game last night? It was such a nailbitter! I'm so glad i was able to watch the grand conclusion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55f9278",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78335fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8f580b4",
   "metadata": {},
   "source": [
    "# Saving model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b4a1973",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at model were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at model and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at model were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at model and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "base_model1 = XLMRobertaForSequenceClassification.from_pretrained(\"model\",num_labels=2,from_tf=False,local_files_only=True)\n",
    "base_model2 = XLMRobertaForSequenceClassification.from_pretrained(\"model\",num_labels=2,from_tf=False,local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37932656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model1.load_state_dict(torch.load('roberta_model.pt'))\n",
    "base_model2.load_state_dict(torch.load('roberta_model_strict.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa46f18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model1.save_pretrained('roberta-mh-nmh-lax')\n",
    "base_model2.save_pretrained('roberta-mh-nmh-strict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e03ba39",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = XLMRobertaForSequenceClassification.from_pretrained('roberta-mh-nmh-lax',num_labels=2,from_tf=False,local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c36bcb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TestPredictor:\n",
    "  def __init__(self,model,tokenizer):\n",
    "    self.model = model\n",
    "    self.tokenizer = tokenizer\n",
    "    # self.model.load_state_dict(torch.load('/content/drive/MyDrive/roberta_model.pt'))\n",
    "#     self.model.load_state_dict(torch.load(model_state))\n",
    "    self.model.eval()\n",
    "\n",
    "  def __call__(self, text, prob=True):\n",
    "    if isinstance(text, str):\n",
    "      text = [text]\n",
    "      unpack = True\n",
    "    else:\n",
    "      unpack = False\n",
    "\n",
    "    tokenized_text = [self.tokenizer.encode(t, add_special_tokens=True) for t in text]\n",
    "    # Pad the tokenized input text to the same length\n",
    "    max_length = max(len(t) for t in tokenized_text)\n",
    "    padded_text = [t + [0] * (max_length - len(t)) for t in tokenized_text]\n",
    "    # Convert the padded input text to PyTorch tensor\n",
    "    input_ids = torch.tensor(padded_text)\n",
    "    # Make predictions with the model\n",
    "    with torch.no_grad():\n",
    "      logits = self.model(input_ids).logits\n",
    "      probs = torch.softmax(logits, dim=1)\n",
    "#       print(logits.argmax())\n",
    "        \n",
    "\n",
    "\n",
    "    if prob:\n",
    "      if unpack:\n",
    "        return (bool(logits.argmax()),probs[0,1].tolist())\n",
    "      else:\n",
    "        return (bool(logits.argmax()),probs[:,1].tolist())\n",
    "    else:\n",
    "      return bool(logits.argmax())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c2c2657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 0.9996656179428101)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tp = TestPredictor(new_model,tokenizer)\n",
    "tp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e4d13c",
   "metadata": {},
   "source": [
    "## Saving worked and test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c75710",
   "metadata": {},
   "source": [
    "#### After this, the model was pushed to hugging face repository since the model size was too big to containerize. Now testing reading the model from hugging face repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc4974b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████| 710/710 [00:00<00:00, 270kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin: 100%|███████| 1.11G/1.11G [04:53<00:00, 3.79MB/s]\n"
     ]
    }
   ],
   "source": [
    "model_from_repo = XLMRobertaForSequenceClassification.from_pretrained(\"GuneetK/mh-nmh-lax\",num_labels=2,from_tf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f981b10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp2 = TestPredictor(model_from_repo,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8b7cb68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 0.9996656179428101)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp2(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4d392d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guneet/opt/miniconda3/envs/bdproj/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, XLMRobertaForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5417a62",
   "metadata": {},
   "source": [
    "# Testing from remotely loaded models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1d3b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestPredictor:\n",
    "  def __init__(self,model,tokenizer):\n",
    "    self.model = model\n",
    "    self.tokenizer = tokenizer\n",
    "    # self.model.load_state_dict(torch.load('/content/drive/MyDrive/roberta_model.pt'))\n",
    "#     self.model.load_state_dict(torch.load(model_state))\n",
    "    self.model.eval()\n",
    "\n",
    "  def __call__(self, text, prob=True):\n",
    "    if isinstance(text, str):\n",
    "      text = [text]\n",
    "      unpack = True\n",
    "    else:\n",
    "      unpack = False\n",
    "\n",
    "    tokenized_text = [self.tokenizer.encode(t, add_special_tokens=True) for t in text]\n",
    "    # Pad the tokenized input text to the same length\n",
    "    max_length = max(len(t) for t in tokenized_text)\n",
    "    padded_text = [t + [0] * (max_length - len(t)) for t in tokenized_text]\n",
    "    # Convert the padded input text to PyTorch tensor\n",
    "    input_ids = torch.tensor(padded_text)\n",
    "    # Make predictions with the model\n",
    "    with torch.no_grad():\n",
    "      logits = self.model(input_ids).logits\n",
    "      probs = torch.softmax(logits, dim=1)\n",
    "#       print(logits.argmax())\n",
    "        \n",
    "\n",
    "\n",
    "    if prob:\n",
    "      if unpack:\n",
    "        return (bool(logits.argmax()),probs[0,1].tolist())\n",
    "      else:\n",
    "        return (bool(logits.argmax()),probs[:,1].tolist())\n",
    "    else:\n",
    "      return bool(logits.argmax())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dae1b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guneet/opt/miniconda3/envs/bdproj/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)okenizer_config.json: 100%|██████| 413/413 [00:00<00:00, 340kB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 17.1M/17.1M [00:02<00:00, 6.87MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████| 280/280 [00:00<00:00, 107kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████| 710/710 [00:00<00:00, 210kB/s]\n",
      "Downloading pytorch_model.bin: 100%|███████| 1.11G/1.11G [02:07<00:00, 8.75MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, XLMRobertaForSequenceClassification\n",
    "import torch\n",
    "\n",
    "base_model1 = XLMRobertaForSequenceClassification.from_pretrained(\"GuneetK/mh-nmh-lax\",num_labels=2,from_tf=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GuneetK/mh-nmh-tokenizer\")\n",
    "base_model2 = XLMRobertaForSequenceClassification.from_pretrained(\"GuneetK/mh-nmh-strict\",num_labels=2,from_tf=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d98f4716",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_test_lax = TestPredictor(base_model1,tokenizer)\n",
    "remote_test_strict = TestPredictor(base_model2,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae65a2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "I know I'm not like everyone else. I don't fit in, and I don't know how to act around people. Sometimes, it feels like I'm an outsider, looking in. But it's not like I want to be this way. I wish I could be normal, and have friends, and go to parties, and not feel so alone all the time.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1e4e138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(True, 0.9996656179428101) (True, 0.9987581968307495)\n"
     ]
    }
   ],
   "source": [
    "print(remote_test_lax(text),remote_test_strict(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbb84561",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Did anyone catch the cannucks' game last night? It was such a nailbitter! I'm so glad i was able to watch the grand conclusion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89afd65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(False, 0.15273839235305786) (False, 0.0002842977410182357)\n"
     ]
    }
   ],
   "source": [
    "print(remote_test_lax(text),remote_test_strict(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cb1ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Wellness watch",
   "language": "python",
   "name": "bdproj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
